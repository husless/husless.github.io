<!doctype html>
<html class="theme-next use-motion theme-next-mist">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />








  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>




<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>


    <meta name="description" content="人生得意须尽欢， 莫使金樽空对月" />



  <meta name="keywords" content="Hexo,next" />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description" content="人生得意须尽欢， 莫使金樽空对月">
<meta property="og:type" content="website">
<meta property="og:title" content="草原上的狼">
<meta property="og:url" content="https://husless.github.io/page/2/index.html">
<meta property="og:site_name" content="草原上的狼">
<meta property="og:description" content="人生得意须尽欢， 莫使金樽空对月">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="草原上的狼">
<meta name="twitter:description" content="人生得意须尽欢， 莫使金樽空对月">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mist',
    sidebar: 'hide'
  };
</script>

  <title> 草原上的狼 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  



  <div class="container one-column 
   page-home 
">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">草原上的狼</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            <i class="menu-item-icon icon-next-about"></i> <br />
            关于
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 
  <section id="posts" class="posts-expand">
    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/07/01/using-scrapy-with-proxies/" itemprop="url">
                Using Scrapy with Proxies
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2015-07-01T00:00:00+08:00" content="2015-07-01">
            2015-07-01
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Python/" itemprop="url" rel="index">
                  <span itemprop="name">Python</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
            &nbsp; | &nbsp;
            <a href="/2015/07/01/using-scrapy-with-proxies/#comments" itemprop="discussionUrl">
              <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2015/07/01/using-scrapy-with-proxies/" itemprop="commentsCount"></span>
            </a>
          </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>当使用爬虫抓取网页上的内容时，很多站点对爬虫不是很有好，会有限制访问者IP等反爬机制。此时，使用代理就是一个非常好的选择。而 Scrapy 提供了 HttpProxyMiddleware 来支持代理。</p>
<p>本文使用的代理IP是从<a href="http://www.kuaidaili.com/free/inha/" target="_blank" rel="external">这里</a>抓取下来的，以 <code>xxx.xxx.xxx.xxx:port</code> 的格式放在 <code>scrapy_project/proxy.txt</code>。然后，Scrapy 从中随机选取一个IP作为 proxy。</p>
<p>1 要使 Scrapy 通过代理进行抓取，首先需要在 <code>settings.py</code> 里面进行相应设置，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Retry when proxies fail</span></span><br><span class="line">RETRY_TIMES = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Retry on most error codes since proxies fail for different reasons</span></span><br><span class="line">RETRY_HTTP_CODES = [<span class="number">500</span>, <span class="number">503</span>, <span class="number">504</span>, <span class="number">400</span>, <span class="number">403</span>, <span class="number">404</span>, <span class="number">408</span>]</span><br><span class="line"></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'scrapy.downloadermiddlewares.retry.RetryMiddleware'</span>: <span class="number">80</span>,</span><br><span class="line">    <span class="string">'scrapy_project.middlewares.ProxyMiddleware'</span>: <span class="number">90</span>,</span><br><span class="line">    <span class="string">'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware'</span>: <span class="number">100</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>2 在 <code>middlewares.py</code> 中定义 <code>ProxyMiddleware</code> 类，将代理IP添加到request.meta中，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span><br><span class="line">This a simple proxy for scrapy. </span><br><span class="line"></span><br><span class="line">The proxy host format like: `http://host:port` or `http://username:password@host:port`</span><br><span class="line">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProxyMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Custom ProxyMiddleware."""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, settings)</span>:</span></span><br><span class="line">        self.proxy_list = settings.get(<span class="string">'PROXY_LIST'</span>)</span><br><span class="line">        <span class="keyword">with</span> open(self.proxy_list) <span class="keyword">as</span> f:</span><br><span class="line">            self.proxies = [ip.strip() <span class="keyword">for</span> ip <span class="keyword">in</span> f]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        request.meta[<span class="string">'proxy'</span>] = <span class="string">'http://&#123;&#125;'</span>.format(random.choice(self.proxies))</span><br></pre></td></tr></table></figure>
<p>另外，还可以简单地通过环境变量来设置代理IP</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> http_proxy = http://ip:port</span><br></pre></td></tr></table></figure>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/06/29/scraping-stackoverflow-with-scrapy-and-mongodb/" itemprop="url">
                Scraping Stackoverflow with Scrapy and MongoDB
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2015-06-29T00:00:00+08:00" content="2015-06-29">
            2015-06-29
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Python/" itemprop="url" rel="index">
                  <span itemprop="name">Python</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
            &nbsp; | &nbsp;
            <a href="/2015/06/29/scraping-stackoverflow-with-scrapy-and-mongodb/#comments" itemprop="discussionUrl">
              <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2015/06/29/scraping-stackoverflow-with-scrapy-and-mongodb/" itemprop="commentsCount"></span>
            </a>
          </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p><a href="http://scrapy.org/" target="_blank" rel="external">Scrapy</a> 是基于 Python 的一个非常流行的网络爬虫框架。本文用 Scrapy 抓取 <a href="http://stackoverflow.com/" target="_blank" rel="external">Stack Overflow</a> 上的问题，<br>被问到得最频繁的排在前面，并将结果存储到 <a href="http://www.mongodb.org" target="_blank" rel="external">MongoDB</a> 中。</p>
<h3 id="环境配置">环境配置</h3><p>需要安装 Scrapy 和 <a href="http://api.mongodb.org/python/current/" target="_blank" rel="external">PyMongo</a>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ mkvirtualenv scrapy</span><br><span class="line">$ pip install scrapy pymongo</span><br></pre></td></tr></table></figure>
<h3 id="创建一个_project">创建一个 project</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy startproject stack</span><br></pre></td></tr></table></figure>
<p>一个 Scrapy project 的目录结构通常如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">├── scrapy<span class="class">.cfg</span></span><br><span class="line">└── stack</span><br><span class="line">    ├── __init__<span class="class">.py</span></span><br><span class="line">    ├── items<span class="class">.py</span></span><br><span class="line">    ├── pipelines<span class="class">.py</span></span><br><span class="line">    ├── settings<span class="class">.py</span></span><br><span class="line">    └── spiders</span><br><span class="line">        └── __init__.py</span><br></pre></td></tr></table></figure>
<h3 id="指定需要抓取的数据">指定需要抓取的数据</h3><p><code>items.py</code> 文件就是用来定义将要抓取的数据的存储“容器”，类似 <code>Python</code> 中的 <code>dict</code>。本文抓取每个问题的<br><code>title</code>、<code>url</code>、<code>tags</code> 和 <code>status</code>四个属性。因此，修改 <code>items.py</code> 文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># http://doc.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.item <span class="keyword">import</span> Item, Field</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StackItem</span><span class="params">(Item)</span>:</span></span><br><span class="line">    title = Field()</span><br><span class="line">    url = Field()</span><br><span class="line">    tags = Field()</span><br><span class="line">    status = Field()</span><br></pre></td></tr></table></figure>
<h3 id="生成Spider">生成Spider</h3><p>通过 Scrapy 内置的模板生成Spider</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy genspider stack_crawler  stackoverflow.com -t crawl</span><br></pre></td></tr></table></figure>
<p>此时，Scrapy project 的目录结构如下</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">├── scrapy<span class="class">.cfg</span></span><br><span class="line">└── stack</span><br><span class="line">    ├── __init__<span class="class">.py</span></span><br><span class="line">    ├── items<span class="class">.py</span></span><br><span class="line">    ├── pipelines<span class="class">.py</span></span><br><span class="line">    ├── settings<span class="class">.py</span></span><br><span class="line">    └── spiders</span><br><span class="line">        ├── __init__<span class="class">.py</span></span><br><span class="line">        ├── stack_crawler.py</span><br></pre></td></tr></table></figure>
<p>修改 <code>stack_crawler.py</code> 文件，定义 <code>parse——item</code> 方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StackCrawlerSpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'stack_crawler'</span></span><br><span class="line">    allowed_domains = [<span class="string">'stackoverflow.com'</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'http://stackoverflow.com/questions?sort=frequent'</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r'questions\?page=[0-9]&amp;sort=frequent'</span>),</span><br><span class="line">             callback=<span class="string">'parse_item'</span>, follow=<span class="keyword">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> href <span class="keyword">in</span> response.xpath(<span class="string">'//div[@class="question-summary"]'</span>):</span><br><span class="line">            url = response.urljoin(href.xpath(<span class="string">'div/h3/a/@href'</span>).extract()[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse_item)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">'title'</span>: response.xpath(<span class="string">'//h1/a/text()'</span>).extract()[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'url'</span>: response.url,</span><br><span class="line">            <span class="string">'tags'</span>: response.xpath(<span class="string">'//a[@class="post-tag"]/text()'</span>).extract(),</span><br><span class="line">            <span class="string">'status'</span>: &#123;</span><br><span class="line">                <span class="string">'votes'</span>: response.xpath(</span><br><span class="line">                    <span class="string">'//div[@class="vote"]/span/text()'</span>).extract()[<span class="number">0</span>],</span><br><span class="line">                <span class="string">'favorite_count'</span>: response.xpath(</span><br><span class="line">                    <span class="string">'//div[@class="favoritecount"]/b/text()'</span>).extract()[<span class="number">0</span>],</span><br><span class="line">                <span class="string">'answers'</span>: response.xpath(</span><br><span class="line">                    <span class="string">'//span[@itemprop="answerCount"]/text()'</span>).extract()[<span class="number">0</span>],</span><br><span class="line">                <span class="string">'views'</span>: response.xpath(</span><br><span class="line">                    <span class="string">'//td/p[@class="label-key"]/b/text()'</span>).extract()[<span class="number">1</span>][:-<span class="number">6</span>],</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p><code>StackCrawlerSpider</code> 类中各变量的意义，从变量名就很容易看出了：</p>
<ul>
<li><code>name</code>  定义 Spider 的名字</li>
<li><code>allowed_domains</code>  是一个列表，列表的每一项是 Spider 允许访问的域名。</li>
<li><code>start_urls</code>  是 Spider 开始抓取的起始 url。</li>
<li><code>rules</code>  定义 Spider 进一步抓取的 url 规则，本例中用来分页抓取前10页的内容。</li>
</ul>
<p>其中， <code>parse_item</code> 方法中的XPath语法，参见 Scrpay 的<a href="http://doc.scrapy.org/en/latest/topics/selectors.html" target="_blank" rel="external">文档</a>以及 <a href="http://www.w3.org/TR/xpath/" target="_blank" rel="external">XPath 文档</a>。</p>
<h3 id="存储设置，pipeline">存储设置，pipeline</h3><p>Scrapy 采用 pipeline 机制来对抓取到的数据进行进一步的分析处理，比如持久化。<br>通过 <code>settings.py</code> 来定义 pipeline 和数据库配置选项：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = [<span class="string">'stack.pipelines.MongoDBPipeline'</span>, ]</span><br><span class="line"></span><br><span class="line">MONGODB_SERVER = <span class="string">"localhost"</span></span><br><span class="line">MONGODB_PORT = <span class="number">27017</span></span><br><span class="line">MONGODB_DB = <span class="string">"stackoverflow"</span></span><br><span class="line">MONGODB_COLLECTION = <span class="string">"questions"</span></span><br></pre></td></tr></table></figure>
<h3 id="Pipeline设置">Pipeline设置</h3><p>Scrapy 通过 pipeline 连接到数据库，在 <code>pipeline.py</code> 中定义：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">from</span> scrapy.conf <span class="keyword">import</span> settings</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoDBPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        client = pymongo.MongoClient(</span><br><span class="line">            settings[<span class="string">'MONGODB_SERVER'</span>],</span><br><span class="line">            settings[<span class="string">'MONGODB_PORT'</span>]</span><br><span class="line">        )</span><br><span class="line">        db = client.get_database(settings[<span class="string">'MONGODB_DB'</span>])</span><br><span class="line">        self.collection = db.get_collection(settings[<span class="string">'MONGODB_COLLECTION'</span>])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        valid = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> item:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> data:</span><br><span class="line">                valid = <span class="keyword">False</span></span><br><span class="line">                <span class="keyword">raise</span> DropItem(<span class="string">'Missing &#123;0&#125;'</span>.format(data))</span><br><span class="line">        <span class="keyword">if</span> valid:</span><br><span class="line">            self.collection.update(&#123;<span class="string">'url'</span>: item[<span class="string">'url'</span>]&#125;,</span><br><span class="line">                                   dict(item), upsert=<span class="keyword">True</span>)</span><br><span class="line">            logging.log(logging.INFO, <span class="string">'Question added to MongoDB database!'</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<h3 id="抓取数据">抓取数据</h3><p>启动 Spider 开始抓取数据。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy crawl stack_crawler</span><br></pre></td></tr></table></figure>
<p>完整代码见<a href="https://github.com/husless/leaning-scrapy" target="_blank" rel="external">Github</a></p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/06/28/configure-ubuntu-server-to-use-wifi/" itemprop="url">
                配置Ubuntu Server 14.04 连接无线WiFi
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2015-06-28T00:00:00+08:00" content="2015-06-28">
            2015-06-28
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Linux/" itemprop="url" rel="index">
                  <span itemprop="name">Linux</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
            &nbsp; | &nbsp;
            <a href="/2015/06/28/configure-ubuntu-server-to-use-wifi/#comments" itemprop="discussionUrl">
              <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2015/06/28/configure-ubuntu-server-to-use-wifi/" itemprop="commentsCount"></span>
            </a>
          </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>服务器版的Ubuntu默认禁止连接WiFi。</p>
<p>用<code>rfkill</code>命令开启：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rfkill unblock all</span><br></pre></td></tr></table></figure>
<p>然后配置<code>/etc/network/interfaces</code>:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">auto wlan0</span><br><span class="line">    iface wlan0 inet dhcp</span><br><span class="line">    wpa-driver wext</span><br><span class="line">    wpa-ssid <span class="string">"TP-LINK_200F82"</span></span><br><span class="line">    wpa-key-mgmt WPA-PSK</span><br><span class="line">    wpa-ap-scan <span class="number">2</span></span><br><span class="line">    wpa-psk <span class="number">9</span>b231391b504add363e12b6d19a9c4eaf52d96eaa91266662c62bfa9aac1529</span><br></pre></td></tr></table></figure>
<p>其中，<code>wpa-ssid</code> 指WiFi信号的SSID； <code>wpa-key-mgmt</code> 表示信号加密格式； <code>wpa-psk</code> 为加密过的WiFi 密码，通过如下命令产生：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wpa_passphrase SSID “password”</span><br></pre></td></tr></table></figure>
<p>重启网络服务，即可连接WiFi 。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/06/27/leap-year/" itemprop="url">
                Leap Year
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2015-06-27T12:23:45+08:00" content="2015-06-27">
            2015-06-27
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Others/" itemprop="url" rel="index">
                  <span itemprop="name">Others</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
            &nbsp; | &nbsp;
            <a href="/2015/06/27/leap-year/#comments" itemprop="discussionUrl">
              <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2015/06/27/leap-year/" itemprop="commentsCount"></span>
            </a>
          </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>直接明了的关于润年的算法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_leap</span><span class="params">(year)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> ((year % <span class="number">4</span> == <span class="number">0</span> &amp;&amp; year % <span class="number">100</span> != <span class="number">0</span>) || year % <span class="number">400</span> == <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/">&laquo;</a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>

 </div>

        

        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="https://avatars1.githubusercontent.com/u/2921692?v=3&s=460" alt="Huszhi" itemprop="image"/>
          <p class="site-author-name" itemprop="name">Huszhi</p>
        </div>
        <p class="site-description motion-element" itemprop="description">人生得意须尽欢， 莫使金樽空对月</p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">14</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">6</span>
              <span class="site-state-item-name">分类</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">20</span>
              <span class="site-state-item-name">标签</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/husless" target="_blank">Github</a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/huszhi" target="_blank">Twitter</a>
              </span>
            
          
        </div>

        
        
          <div class="cc-license motion-element" itemprop="license">
            <a href="http://creativecommons.org/licenses/by-nc-sa/4.0" class="cc-opacity" target="_blank">
              <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp;  2014 - 
  <span itemprop="copyrightYear">2015</span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Huszhi</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    
    

  

    <script type="text/javascript">
      var disqus_shortname = 'huszhi';
      var disqus_identifier = 'page/2/index.html';
      var disqus_title = '';
      var disqus_url = '';

      function run_disqus_script(disqus_script){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');
      
    </script>
  


  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  

  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  

  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
</body>
</html>
